# Snowflake DDL: production-ready starter scripts and guidance

Below is a focused, implementation-ready Snowflake DDL package you can use as a starting point for the Hospital Supply Chain & Inventory Management project. It includes database/schema setup, roles and grants, file format and stage examples, core table DDL for canonical dimensions and facts, Streams/Tasks/Snowpipe patterns for incremental ingestion, and brief notes on clustering and security.

Where I make claims about Snowflake features or recommended patterns, I cite official Snowflake docs. ([Snowflake Documentation][1])



## Quick notes before you run anything

* Replace placeholders like `<ACCOUNT>`, `<S3_BUCKET>`, `<AWS_IAM_ROLE_ARN>`, `<USER_NAME>`, and `<WAREHOUSE_NAME>` with your tenant-specific values.
* This script assumes Snowflake standard SQL and typical privileges. Adjust role names and privileges to match your org.
* The scripts are idempotent where practical (use IF NOT EXISTS). For production use, incorporate into your deployment pipeline and secret manager for credentials.
* Streams, Tasks, and Snowpipe are used to support incremental CDC-style processing. See Streams and Tasks docs for semantics and behavior. ([Snowflake Documentation][2])



# 1. Create database, schemas, warehouses, and roles

```sql
-- Database and schemas
CREATE DATABASE IF NOT EXISTS hospital_supply_chain;

USE DATABASE hospital_supply_chain;

CREATE SCHEMA IF NOT EXISTS raw;
CREATE SCHEMA IF NOT EXISTS staging;
CREATE SCHEMA IF NOT EXISTS silver;
CREATE SCHEMA IF NOT EXISTS gold;
CREATE SCHEMA IF NOT EXISTS ops;

-- Warehouse (example)
CREATE WAREHOUSE IF NOT EXISTS wh_etl
  WAREHOUSE_SIZE = 'XSMALL'
  AUTO_SUSPEND = 60
  AUTO_RESUME = TRUE
  INITIALLY_SUSPENDED = TRUE;

-- Roles and users (example roles only)
CREATE ROLE IF NOT EXISTS role_data_engineer;
CREATE ROLE IF NOT EXISTS role_data_analyst;
CREATE ROLE IF NOT EXISTS role_data_viewer;
CREATE ROLE IF NOT EXISTS role_data_qa;
```



# 2. File format and external stage example (S3)

```sql
-- File format for CSV landing files
CREATE FILE FORMAT IF NOT EXISTS ops.csv_format
  TYPE = 'CSV'
  FIELD_DELIMITER = ','
  SKIP_HEADER = 1
  NULL_IF = ('', 'NULL');

-- Example external stage (S3). Use cloud-provider auth or storage integration in production.
CREATE STAGE IF NOT EXISTS ops.s3_ingest_stage
  URL = 's3://<S3_BUCKET>/incoming/'
  FILE_FORMAT = ops.csv_format
  STORAGE_INTEGRATION = '<YOUR_STORAGE_INTEGRATION>'; -- recommended for secure access
```



# 3. Raw-layer tables (landing, keep raw payloads)

```sql
-- Raw landing table: keep as close to source shape as possible.
CREATE TABLE IF NOT EXISTS raw.inventory_landing (
  ingest_id          STRING AUTOINCREMENT,   -- unique for traceability
  source_system      STRING,
  file_name          STRING,
  file_row_number    NUMBER,
  raw_payload        VARIANT,                -- raw JSON parsed (if JSON) or CSV parsed to JSON
  loaded_at          TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP
);

-- Option: raw CSV parsed into structured columns (if you prefer structured landing)
CREATE TABLE IF NOT EXISTS raw.inventory_landing_struct (
  ingest_id          STRING AUTOINCREMENT,
  source_system      STRING,
  reference_id       STRING,
  item_sku           STRING,
  location_code      STRING,
  quantity           NUMBER,
  unit_of_measure    STRING,
  transaction_type   STRING, -- receipt, issue, adjustment
  transaction_ts     TIMESTAMP_LTZ,
  lot_number         STRING,
  expiry_date        DATE,
  loaded_at          TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP
);
```



# 4. Staging tables (canonical typed staging used by dbt / merges)

```sql
CREATE TABLE IF NOT EXISTS staging.stg_inventory_transactions (
  staging_id         STRING AUTOINCREMENT,
  source_system      STRING,
  reference_id       STRING,      -- source unique id for idempotency
  item_sku           STRING,
  item_description   STRING,
  location_code      STRING,
  quantity           NUMBER,
  uom                STRING,
  transaction_type   STRING,      -- receipt, issue, adjustment
  transaction_ts     TIMESTAMP_LTZ,
  lot_number         STRING,
  expiry_date        DATE,
  load_batch_id      STRING,      -- helpful for tracing a file or batch
  loaded_at          TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP,
  raw_ingest_id      STRING
);
```



# 5. Core dimensional and fact tables (Gold / canonical)

```sql
-- dim_item (SCD Type 2 pattern fields included)
CREATE TABLE IF NOT EXISTS gold.dim_item (
  item_id            STRING AUTOINCREMENT,
  sku                STRING NOT NULL,
  description        STRING,
  manufacturer       STRING,
  uom                STRING,
  category           STRING,
  shelf_life_days    NUMBER,
  effective_from     TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP,
  effective_to       TIMESTAMP_LTZ,
  is_current         BOOLEAN DEFAULT TRUE,
  created_at         TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP
);

CREATE UNIQUE INDEX IF NOT EXISTS idx_dim_item_sku ON gold.dim_item(sku) WHERE is_current = TRUE;

-- dim_location
CREATE TABLE IF NOT EXISTS gold.dim_location (
  location_id        STRING AUTOINCREMENT,
  location_code      STRING NOT NULL,
  name               STRING,
  facility           STRING,
  building           STRING,
  floor              STRING,
  department         STRING,
  created_at         TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP
);

-- dim_supplier
CREATE TABLE IF NOT EXISTS gold.dim_supplier (
  supplier_id        STRING AUTOINCREMENT,
  supplier_code      STRING,
  supplier_name      STRING,
  lead_time_days     NUMBER,
  contact_info       STRING,
  created_at         TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP
);

-- dim_time (optional, but handy)
CREATE TABLE IF NOT EXISTS gold.dim_time (
  date_key           DATE PRIMARY KEY,
  year               NUMBER,
  month              NUMBER,
  day                NUMBER,
  week_of_year       NUMBER,
  fiscal_month       NUMBER,
  fiscal_year        NUMBER
);

-- fact_inventory_transaction (granular)
CREATE TABLE IF NOT EXISTS gold.fact_inventory_transaction (
  transaction_id     STRING AUTOINCREMENT,
  reference_id       STRING,         -- source reference for idempotency
  item_id            STRING,
  sku                STRING,
  location_id        STRING,
  location_code      STRING,
  transaction_type   STRING,
  quantity           NUMBER,
  unit_cost          NUMBER,
  transaction_ts     TIMESTAMP_LTZ,
  lot_number         STRING,
  expiry_date        DATE,
  source_system      STRING,
  created_at         TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP
);

-- fact_purchase_order (simplified)
CREATE TABLE IF NOT EXISTS gold.fact_purchase_order (
  po_line_id         STRING AUTOINCREMENT,
  po_number          STRING,
  supplier_id        STRING,
  sku                STRING,
  quantity_ordered   NUMBER,
  quantity_received  NUMBER,
  order_date         DATE,
  expected_date      DATE,
  status             STRING,
  created_at         TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP
);

-- fact_expiration_event
CREATE TABLE IF NOT EXISTS gold.fact_expiration_event (
  expiration_event_id STRING AUTOINCREMENT,
  item_id             STRING,
  sku                 STRING,
  location_id         STRING,
  lot_number          STRING,
  expiry_date         DATE,
  days_to_expiry      NUMBER,
  disposition         STRING,
  created_at          TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP
);
```



# 6. Streams and Tasks: incremental merge pattern

Below pattern uses a stream on the staging table, a task to run a merge into gold.fact_inventory_transaction, and ordering via reference_id for idempotent MERGE.

Create stream on the staging table:

```sql
USE SCHEMA staging;

CREATE OR REPLACE STREAM stg_inventory_tx_stream
  ON TABLE staging.stg_inventory_transactions
  SHOW_INITIAL_ROWS = FALSE;  -- only capture changes after stream creation
```

Create a task that merges changes from the stream into gold.fact_inventory_transaction. Note: a task runs with a warehouse or serverless option. See docs for scheduling and task graphs. ([Snowflake Documentation][3])

```sql
USE SCHEMA staging;

CREATE OR REPLACE TASK task_merge_inventory_stream
  WAREHOUSE = wh_etl
  SCHEDULE = 'USING CRON  * * * * * UTC'  -- example: run every minute; adjust as required
AS
  MERGE INTO gold.fact_inventory_transaction tgt
  USING (
    SELECT *
    FROM staging.stg_inventory_transactions
    WHERE (source_system, reference_id, transaction_ts) IN (
      SELECT source_system, reference_id, transaction_ts FROM staging.stg_inventory_transactions
    )
  ) src
  ON tgt.reference_id = src.reference_id AND tgt.source_system = src.source_system
  WHEN MATCHED AND src.quantity = 0 THEN
    DELETE
  WHEN MATCHED THEN
    UPDATE SET
      sku = src.item_sku,
      location_code = src.location_code,
      quantity = src.quantity,
      transaction_ts = src.transaction_ts,
      lot_number = src.lot_number,
      expiry_date = src.expiry_date,
      created_at = CURRENT_TIMESTAMP()
  WHEN NOT MATCHED THEN
    INSERT (reference_id, sku, location_code, quantity, transaction_ts, lot_number, expiry_date, source_system, created_at)
    VALUES (src.reference_id, src.item_sku, src.location_code, src.quantity, src.transaction_ts, src.lot_number, src.expiry_date, src.source_system, CURRENT_TIMESTAMP());
```

After creating the task, enable it:

```sql
ALTER TASK task_merge_inventory_stream RESUME;
```

Notes:

* Use task graphs to create ordered pipelines if you have multiple dependent merges. ([Snowflake Documentation][4])
* You can choose to run tasks serverless (omit WAREHOUSE) if available in your edition and region. Check account capabilities.



# 7. Snowpipe: auto-ingest from stage to raw table

Example `CREATE PIPE` for auto-loading CSV files into `raw.inventory_landing_struct`. Ensure S3 event notifications are configured to notify Snowpipe. See Snowpipe docs for setup details. ([Snowflake Documentation][5])

```sql
CREATE OR REPLACE PIPE raw.pipe_inventory_landing
  AUTO_INGEST = TRUE
  AS
  COPY INTO raw.inventory_landing_struct
  FROM @ops.s3_ingest_stage
  FILE_FORMAT = (FORMAT_NAME = ops.csv_format)
  ON_ERROR = 'CONTINUE';
```

After this, S3 event notifications or Storage Integration will trigger the pipe to load new files.



# 8. Grants and access control (example)

```sql
-- Grant role permissions on schemas and tables
GRANT USAGE ON DATABASE hospital_supply_chain TO ROLE role_data_engineer;
GRANT USAGE ON SCHEMA hospital_supply_chain.raw TO ROLE role_data_engineer;
GRANT USAGE ON SCHEMA hospital_supply_chain.staging TO ROLE role_data_engineer;
GRANT USAGE ON SCHEMA hospital_supply_chain.gold TO ROLE role_data_engineer;

GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA staging TO ROLE role_data_engineer;
GRANT SELECT ON ALL TABLES IN SCHEMA gold TO ROLE role_data_analyst;

-- Future grants to ensure new objects inherit privileges
GRANT SELECT ON FUTURE TABLES IN SCHEMA gold TO ROLE role_data_analyst;
GRANT USAGE ON WAREHOUSE wh_etl TO ROLE role_data_engineer;
```



# 9. Clustering and performance notes

* For high-volume fact tables, consider clustering keys on (`sku`, `transaction_ts`) or a compound key that matches query patterns. Use Snowflake clustering to improve pruning for large tables. See table design considerations and best practices. ([Snowflake Documentation][1])
* Partitioning is handled automatically in Snowflake; clustering improves performance for certain predicates but adds maintenance cost. Evaluate with representative workload and use `SYSTEM$CLUSTERING_INFORMATION` to monitor. ([Snowflake Documentation][1])



# 10. Security and compliance flags

* As a legal requirement, ensure a signed Business Associate Agreement before storing any PHI in Snowflake. Confirm your account edition and features for HIPAA support. ([Snowflake Documentation][6])
* For customer-managed keys and additional encryption controls, review Snowflake Tri-Secret Secure and CMK options if required. ([Snowflake Documentation][7])



# 11. Reasoning steps (concise)

* Designed an immutable raw landing area to capture source payloads for traceability.
* Added typed staging tables to make merges deterministic and to be consumed by dbt models.
* Provided canonical gold dimension and fact DDL suitable for star-schema analytics in Power BI.
* Added Streams, Tasks, and Snowpipe examples to support near-real-time ingestion and CDC-style workflows. ([Snowflake Documentation][2])



# 12. Points that require verification or remain uncertain

* Storage integration details and S3 event notification configuration for Snowpipe. (Verify with infra team and Snowflake account settings). ([Snowflake][8])
* Whether your Snowflake account supports serverless Tasks or requires a dedicated warehouse; this depends on edition/region. (Verify account features). ([Snowflake Documentation][3])
* Business Associate Agreement (BAA) status and whether PHI will be stored. This is an organizational/legal check. ([Snowflake Documentation][6])
* Exact source field names, file formats, and sample volumes so that types, sizes, and clustering keys can be tuned.
* Warehouse sizing and concurrency expectations to estimate credits and performance costs.



# 13. Suggested next actions (pick one)

* I can generate dbt `models/` skeleton SQL files that implement the merges and SCD Type 2 logic using the staging and gold tables above.
* I can produce a Snowflake-ready deployment script (single file) with placeholders replaced by environment variables and comments for CICD.
* I can run a short sizing checklist template you can fill with sample daily transactions and concurrency to estimate warehouse sizing and cost.

Tell me which of the three you want and I will generate it immediately.

[1]: https://docs.snowflake.com/en/user-guide/table-considerations?utm_source=chatgpt.com "Table Design Considerations"
[2]: https://docs.snowflake.com/en/user-guide/streams-intro?utm_source=chatgpt.com "Introduction to Streams"
[3]: https://docs.snowflake.com/en/user-guide/tasks-intro?utm_source=chatgpt.com "Introduction to tasks"
[4]: https://docs.snowflake.com/en/user-guide/tasks-graphs?utm_source=chatgpt.com "Create a sequence of tasks with a task graph"
[5]: https://docs.snowflake.com/en/user-guide/data-load-snowpipe-intro?utm_source=chatgpt.com "Snowpipe"
[6]: https://docs.snowflake.com/en/user-guide/intro-editions?utm_source=chatgpt.com "Snowflake editions"
[7]: https://docs.snowflake.com/en/user-guide/security-encryption-tss?utm_source=chatgpt.com "Tri-Secret Secure in Snowflake"
[8]: https://www.snowflake.com/en/developers/guides/getting-started-with-snowpipe/?utm_source=chatgpt.com "Getting Started with Snowpipe"